# This is a configuration file to create a Krypton pool

image: 
  repository: <KRYPTON_IMAGE_REPOSITORY>
  tag: "4.5.0-gl557693"
  pullPolicy: IfNotPresent
configFileName: Krypton
##Configure imagePullSecrets to access private docker registry
imagePullSecrets: []
  #  - name: secret1
##Number of krypton replica count
replicas: 1
maxReplicas: 10
##strategy.rollingUpdate.maxUnavailable field specifies the maximum number of Pods that can be unavailable during the update process
##Configure suitable value depending on the environment 
strategy:
  rollingUpdate:
    maxUnavailable: 0
##Node taints    
tolerations: []    
##Affinity configurations
affinity: {}
##Node selectors
nodeSelector: {} 
  #app: krypton
  #krypton-eng-usa: krypton-eng-usa
#Resource memory metrics config should be configured based on the datapack size. Any change in krypton.resources.* 
#properties coupled with helm upgrade will trigger rolling update.
resources: {}
  #limits:
  #  cpu: 400m
  #  memory: 3G
  #requests:
  #  cpu: 125m
  #  memory: 300M

##Additional environment for krypton
#environment:
#- name: SECRET_USERNAME
#  valueFrom:
#    secretKeyRef:
#      name: mysecret
#      key: username

##--------------------------------------------------------------------------------------------------------------------------------
##AutoScaling configuration for krypton. 
##Works effectively if all containers have request and limits defined in resources.
##To use contanst maxReplicas, double quote the value ex: "5"
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: "{{ mul .Values.autoscaling.minReplicas 10 }}"
  targetCPUUtilizationPercentage: 65
  annotations: {}

##--------------------------------------------------------------------------------------------------------------------------------

readiness:
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 10

liveness:
  initialDelaySeconds: 240
  periodSeconds: 10
  timeoutSeconds: 10
  failureThreshold: 3

##--------------------------------------------------------------------------------------------------------------------------------

##runtimeConfig will change the runtime configmap values on helm upgrade. Krypton instances will adapt to the change 
##without restart
runtimeConfig:
  log:
    level: status

##--------------------------------------------------------------------------------------------------------------------------------

https: 
  enable: false
  port: 8080
  #secretName:
  sslConfig: {}
##keyFile and certFile should be placed in SSLCertificates directory
  #  keyFile: './SSLCertificates/server.key'
  #  certFile: './SSLCertificates/server.crt'
  #  passphrase:
  #  requestCert: false
  #  rejectUnauthorized: true
  #  caCertificates:

http: 
  enable: true
  port: 8078

http2:
  enable: true
  port: 8090

##--------------------------------------------------------------------------------------------------------------------------------
deployment:
  annotations: {}
  deploymentLabels: {}
  serviceLabels: {}
  podLabels: {}
  podAnnotations: {}
runtimeConfigmap:
  annotations: {}
configmap:
  annotations: {}
secrets:
  annotations: {}
networkPolicies:
  annotations: {}
##--------------------------------------------------------------------------------------------------------------------------------
service:
  type: ClusterIP
## uncomment to make the Krypton service headless 
  #clusterIP: None
  http2Port: 8090
  httpPort: 8078
  httpsPort: 8080
  annotations: {}
   # prometheus.io/scrape: 'true'
   # prometheus.io/port: '8078'
   # prometheus.io/scheme: 'http'
   # prometheus_io_path: '/metrics'
##--------------------------------------------------------------------------------------------------------------------------------
ingress:
  annotations:
    kubernetes.io/ingress.class: nuance-coretech
    ingress.kubernetes.io/ssl-passthrough: "false"
    ingress.kubernetes.io/rewrite-target: "/"
    ingress.kubernetes.io/balance-algorithm: "leastconn"
    ingress.kubernetes.io/ssl-redirect: "false"
##If not configured the path is same as helm release name
#  path: 
##Ingress host configuration for virtual host based routing 
#  host: krypton.local
#  tls:
#  - hosts:
#    - krypton.local

##--------------------------------------------------------------------------------------------------------------------------------
prometheusOperator:
  enable: false
  namespace: monitoring
  serviceMonitor:
    labels:
      team: coretech
    config:
      interval: 10s
      #scheme: https
      #tlsConfig:
      #  caFile: 
      #  certFile:
      #  insecureSkipVerify: true
      #  keyFile:
  prometheusRulesLabels:
    prometheus: prometheus
    role: alert-rules

prometheus:
  alerts:
    selectorLabels: {}
##--------------------------------------------------------------------------------------------------------------------------------
##diagnosticLogs.enableFile should be set to true when fluentBitSideCar is enabled
fluentBitSideCar:
  enabled: false
  resources: {}
  image: 
    repository: fluent/fluent-bit
    tag: "1.0"
    pullPolicy: IfNotPresent
  kvslua: |-
      function parse_kvs(tag, timestamp, record)
         if record.kvs == nil then
           return 0
         end
         -- Modify record in place
         for k, v in string.gmatch(record.kvs, "([^,=]+)=([^,=]+)") do
             record[k] = v
         end
         record.kvs = nil
         return 1, timestamp, record
      end
  parser: |-
      [PARSER]
          Name                      krypton-diag-logs
          Format                    regex
          Regex                     ^(?<timestamp>[^*]+) (-|\{(?<kvs>[^}]*)\}) (?<level>[^:]+): (?<message>.*)$
          Time_Key                  timestamp
          Time_Format               %Y-%m-%d %H:%M:%S,%LZ	
  service: |-
      [SERVICE]
          Parsers_File parser.conf
          storage.backlog.mem_limit 30M
          HTTP_Listen               0.0.0.0
          HTTP_Port                 7700
          HTTP_Server               true
  input: |-
      [INPUT]
          Name                      tail
          Path                      /var/local/Nuance/Krypton/logs/*.log*
          DB                        /var/local/Nuance/Krypton/logs/diag-logs-fluentbit.db
          Tag                       diagnostic_logs
  filter: |-
      [FILTER]
          Name                      parser
          Match                     diagnostic_logs
          Key_Name                  log
          Parser                    krypton-diag-logs
      [FILTER]
          Name                      lua
          Match                     diagnostic_logs
          script                    kvs.lua
          call                      parse_kvs
      [FILTER]
          Name                      record_modifier
          Record POD_NAME           ${MY_POD_NAME}
          Record POD_NAMESPACE      ${MY_POD_NAMESPACE}
          Record NODE_NAME          ${MY_NODE_NAME}
          Record POD_IP             ${MY_POD_IP}
          Record HELM_RELEASE_NAME  ${HELM_RELEASE_NAME}
          Record SERVICE            ${SERVICE}
          Record IMAGE              ${IMAGE}
          Match                     diagnostic_logs
  output: |-
      #[OUTPUT]
      #    name          splunk
      #    host          <SPLUNK_HOST>
      #    port          <SPLUNK_PORT>
      #    tls           off
      #    tls.verify    off
      #    splunk_token  <SPLUNK_TOKEN>
      #    Match         diagnostic_logs
      #[OUTPUT]
      #    name          es
      #    host          <ELASTICSEARCH_HOST>
      #    port          <ELASTICSEARCH_PORT>
      #    tls           off
      #    tls.verify    off
      #    Index         krypton
      #    Match         diagnostic_logs
##--------------------------------------------------------------------------------------------------------------------------------  

##fluentd configuration
fluentd:
  enabled: false
  resources: {}
  image:
    repository: ncr.nuance.com/xfabric-docker-dev/fluentd
    tag: 1.9.2-1-gl537420
    pullPolicy: IfNotPresent
  service: |-
    <source>
      @type tail
      path /var/local/Nuance/Krypton/logs/*.calllog*
      pos_file /fluentd/etc/log.pos
      tag call_log
      <parse>
        @type json
      </parse>
    </source>
    <source>
     @type http
     port 7701
     bind 0.0.0.0
    </source>
    # expose metrics in prometheus format
    <source>
      @type prometheus
      bind 0.0.0.0
      port 24231
      metrics_path /metrics
    </source>
    <source>
      @type prometheus_output_monitor
      interval 10
      <labels>
        hostname ${hostname}
      </labels>
    </source>
    <source>
      @type prometheus_monitor
      interval 10
      <labels>
        hostname ${hostname}
      </labels>
    </source>
    <source>
      @type prometheus_tail_monitor
      interval 10
      <labels>
        hostname ${hostname}
      </labels>
    </source>
    <match call_log>
      @type copy
      <store>
        @type rdkafka2
        brokers fabric-dev-eventhub.servicebus.windows.net:9093
        #use_event_time true
        <format>
          @type json
        </format>
        topic_key appid
        default_topic null
        default_message_key null
        rdkafka_delivery_handle_poll_timeout 0
        <buffer appid>
          @type file
          path /tmp/buffer
          flush_interval 0s
        </buffer>
        rdkafka_options {
          "log_level" : 7,
          "security.protocol" : "SASL_SSL",
          "sasl.mechanisms" : "PLAIN",
          "sasl.username" : "$ConnectionString",
          "sasl.password" : "Endpoint=sb://fabric-dev-eventhub.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=F8Dr0rVNX5vCeuqzKUhPvKAWZxyfgHtPpvHzKkP9idI="
        }
      </store>
      <store>
        @type elasticsearch
        host release-elasticsearch-client.fabric-stage
        port 9200
        logstash_format true
        logstash_prefix asraas
      </store>
      <store>
        @type stdout
      </store>
    </match>
##--------------------------------------------------------------------------------------------------------------------------------
jaegerAgentSideCar:
  enabled: false
  image: jaegertracing/jaeger-agent
  tag:  1.11.0
  pullPolicy: IfNotPresent
  cmdlineParams: {}
  config:
    collectorHostPort: <host>:<port>
    # zipkinThriftPort :accept zipkin.thrift over compact thrift protocol
    zipkinThriftPort: 5775
    # compactPort: accept jaeger.thrift over compact thrift protocol
    compactPort: 6831
    # binaryPort: accept jaeger.thrift over binary thrift protocol
    binaryPort: 6832
    # samplingPort: (HTTP) serve configs, sampling strategies
    samplingPort: 5778
  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    # requests:
    #   cpu: 256m
    #   memory: 128Mi
##--------------------------------------------------------------------------------------------------------------------------------
diagnosticLogs:
  enableFile: false
  maxSize: 5242880
  maxFiles: 5
##--------------------------------------------------------------------------------------------------------------------------------  
#pCIKeySecretName:
##-------------------------------------------------------------------------------------------------------------------------------- 
grafana:
  enabled: false
  configmap:
    annotations: {}
  selectorLabels: 
    grafana_dashboard: "grafana_dashboard"
  haproxyMetricsService: commons-global-exporter
  datasource: 
    name: "Prometheus"
    label: "Prometheus"
    description: ""
    type: "datasource"
    pluginId: "prometheus"
    pluginName: "Prometheus"
##-------------------------------------------------------------------------------------------------------------------------------- 
config:
  enableInfoEvents: true
  enableCallLogEvents: true
  shutdownTimeoutMs: 10000 
  #maxAudioChunkDuration:
  #pushGatewayURL:
  protocol:
    defaultVersion: '3.0'
    generateAttachId: false
    acknowledgeAudio: false
    tokenizationKey:
    enableMetrics: true
    defaultTopic: 'GEN'
    defaults:
      recognitionParameters:
        enableSpeakerProfileUpdate: true
        enableNBestOnRejection: false
      sessionParameters:
        detachedTimeout: '0ms'
  additionalInitializeParameters:
    DisableSharedMemory: false
##License server configuration, change in license path coupled with helm release upgrade will trigger rolling update
  license:
    path: <LICENSE_SERVER_PORT>@<LICENSE_SERVER_HOST>
##Krypton Datapacks configuration for the pool, change in dataPack.languages coupled with helm release upgrade
##will trigger rolling update
  dataPack:
    languages:
    - <DATA_PACK_TO_BE_LOADED>
    loadNightlyUpdateVersionIfAvailable: true
  #minioClients:
  #  - url: <minio_server_url>
  #    accessKey: <obfuscated_accessKey>
  #    secretKey: <obfuscated_secretKey>
  #    iamRole:   
  #    requestTimeoutMs: 10000
  #    saveTimeoutMs: 10000
  #    serverName:
  #callRecording:
  #  serverURL: <call_log_aggregator_url>
## callRecording.PCIKey should be present in PCIKeys directory
  #  PCIKey: PCIKeys/call_recording_PCI_public_key.pem
  #  includeAudio: true
  #  includeS3Events: true
  #  includeS3Configuration: true
  #  includeMetrics: true
  #  includePrePostProcessingResults: true
  #  includeEnvVar:
  #  - SWISRSDK
## applicationName.PCIKey should be present in PCIKeys directory
  #  applications: 
  #     - applicationName: exampleApp
  #       PCIKey: PCIKeys/exampleApp_PCI_public_key.pem
  #       includeAudio: true
  httpClient:
    requestTimeoutMs: 10000
    followRedirect: true
    maxRedirects: 10
    rejectUnauthorized: false
    keyFile:
    certFile:
    passphrase:
    caCertificates: []
    cache:
        enable: true
        maxSize: 500
        cleanAfterNumSessionsCompleted: 5
        saveToDisk: false
  enableRuntimeConfig: true
  grpc:
    protoFiles:
     - v1beta1/nuance_asr.proto
     - v1beta2/nuance_asr.proto
     - v1/nuance/asr/v1/recognizer.proto
    websocketPath: /grpc
    maxAudioPrebufferSize: 320000
  routerNotifier:
    enable: false
    advertise: http
    leakCheckIntervalMs: 30000
    leakRecoveryDelayMs: 1000
    redisClient:
        host: 
        port: 6379
        enableOfflineQueue: false
  tracing:
    enable: false
    serviceName:
    reporter:
      agentHost:
      agentPort:
      collectorEndpoint:
      username:
      password:
      logSpans: false
      flushIntervalMs:
    sampler:
      type:
      param: 0
      hostPort:
      refreshIntervalMs:
    throttler:
      host:
      port:
      refreshIntervalMs:
  preload:
    - dataPack: 
        language:
        topic:
      objects:
        - url: sampleURL
          weight: 0
          type: application/x-nuance-domainlm
##CCSS URI-to-URL resolver configuration
  uriResolver:
    templates:
      get: <URI_TO_GET_URL>
##--------------------------------------------------------------------------------------------------------------------------------
##Persistence Storage Configurations 
##PV should support ReadOnlyMany access mode 
persistence:
##volumeType can be nfs, hostPath, other, existingClaim.
  volumeType: nfs 
##Storage property is applicable to volume type nfs and other only 
  storage: 20Gi
##---------------------------------------------------------------------------------------------------------------------------------
##If volumeType is NFS. 
##PV and PVC will be created for every pool if NFS is enabled. NFS server details needs to be configured below
##Configured dataPack.languages should be available on the NFS mount path
  nfs:
##configure nfs server IP 
    server: <NFS_SERVER_IP>
##nfs server mount path
    path: <NFS_MOUNT_PATH>
##---------------------------------------------------------------------------------------------------------------------------------
##If volumeType is hostPath.
##hostMountPath property is required. Configured dataPack.languages should be available on the host mount path
  hostMountPath: <HOST_PATH>
#---------------------------------------------------------------------------------------------------------------------------------
##If volumeType is other. Configured dataPack.languages should be available on the PV mount 
  other:
    storageClassName: ""
##pvSelectors can be configured to bind PVC to a static configured  PV
    #pvSelectors: 
    #  pvLabel: pvLabelValue
##---------------------------------------------------------------------------------------------------------------------------------
##If volumeType is existingClaim. Configured dataPack.languages should be available on the PV mount 
  existingClaim:

##-------------------------------------------------------------------------------------------------------------------------------- 
##-------------------------------------------------------------------------------------------------------------------------------- 
haproxyGrpc:
  enable: true
  image: 
    repository: haproxy
    tag: 1.9.6
    pullPolicy: IfNotPresent 
  strategy:
    rollingUpdate:
      maxUnavailable: 0
  affinity: {}
  nodeSelector: {}
  resources: {}
  deployment:
    annotations: {}
    podLabels: {}
    podAnnotations: {}
  configmap:
    annotations: {}
  service:
    type: ClusterIP
    #nodePort: 32355
    annotations: {}
    port: 80
  config:
    defaults: |-
        defaults
          log global
          timeout client 50s
          timeout client-fin 50s
          timeout connect 50s
          timeout server 50s
          timeout tunnel 50s
          mode http
    dnsResolver: |-
        resolvers k8s
          nameserver dns1 <DNS_RESOLVER_IP>
          hold timeout         30s
          hold refused         30s
          accepted_payload_size 8192

##-------------------------------------------------------------------------------------------------------------------------------- 
##Alert Configurations
alertConfigurations:
  enableAlerts: false
  configmap:
    annotations: {}
  alertLabels:
    alert_through: pagerduty-platform    
  failedCommands:
    timeInterval: 15m
    percentageThreshold: 5
    timeRange: 1m
  protocolFailure:
    timeInterval: 15m
    percentageThreshold: 5
    timeRange: 1m
  recognitionObjectLoadFailure:
    timeInterval: 15m
    percentageThreshold: 5
    timeRange: 1m
  createSessionFailure:
    timeInterval: 15m
    percentageThreshold: 5
    timeRange: 1m
  haproxy5xxResponse:
    timeInterval: 15m
    countThreshold: 0
    timeRange: 5m
  oovLimitFailure:
    timeInterval: 15m
    countThreshold: 0
    timeRange: 1m
  initializationFailure:
    timeInterval: 15m
    timeInSeconds: 600
  configurationFailure:
    timeInterval: 15m
    timeInSeconds: 600
  persistentErrorFailure:
    timeInterval: 15m
    timeInSeconds: 600
  recognitionObjectNotAffectingRecognition:
    timeInterval: 15m
    countThreshold: 0
    timeRange: 1m
  callRecorderErrorResponse:
    timeInterval: 15m
    timeRange: 5m
    percentageThreshold: 10
  callRecorderResponseTimeout:
    timeInterval: 15m
    timeRange: 5m
    percentageThreshold: 10
  callRecorderIncompleteSessions:
    timeInterval: 15m
    timeRange: 5m
    percentageThreshold: 10
  callRecorderConnectionErrors:
    timeInterval: 15m
    timeRange: 5m
    percentageThreshold: 10
  licenseServerDisconnect:
    timeInterval: 15m
  podRestart:
    timeInterval: 15m
    timeRange: 30m
    avgThresholdCount: 2  
  grpcFailedResponse:
    timeInterval: 15m
    timeRange: 1d
    percentageThreshold: 75 

##--------------------------------------------------------------------------------------------------------------------------------
###Network Policies Configuration
#Default policies are to DENY ALL traffic except the DNS
#Traffic From - Prometheus, rmRegistration, Router/HAProxy, Ambassador
#Traffic To - PushGateway, Jaeger, rmRegistration, Router, Redis, CCSS, Artifactory, MinIO, Clag    
networkPolicy:
  defaultEnabled: false
  enabled: false
  monitoring:
    prometheus:
      enabled: false
      namespaceSelector:
        matchLabels: {}
      podSelector:
        matchLabels: {}
    pushgateway:
      enabled: false
      namespaceSelector:
        matchLabels: {}
      podSelector:
        matchLabels: {}
    jaeger:
      enabled: false
      namespaceSelector:
        matchLabels: {}
      podSelector:
        matchLabels: {}
  redis:
    enabledovernamespace: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}
    enabledoveripport: false 
    enabledoverport: false 
    ipValue:
    portValue:      
  ccss:
    enabled: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}    
  artifactory:
    enabledovernamespace: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}
    enabledoverip: false
    ipValue:
  router:
    enabled: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}
  ambassador:
    enabled: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}      
  minio:
    enabled: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}
  clag:
    enabled: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}  
  rmRegistration:
    enabled: false
    namespaceSelector:
      matchLabels: {}
    podSelector:
      matchLabels: {}
  genericPorts:
    enabled: false
    tcpPortValues: []
    udpPortValues: []

