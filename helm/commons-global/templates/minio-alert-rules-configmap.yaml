{{- if eq .Values.minio.enabled true}}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "minio.fullname" . }}-alerts
  labels:
    alerts: minio
    app: {{ template "minio.name" . }}-alerts
    chart: {{ template "minio.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  minio.rules.yaml: |-
    groups:
    - name: ./minio-alert.rules-{{ .Release.Name }}
      rules:
      - alert: minio-get_latency_alert-{{ .Release.Name }}
        expr: sum by (service) (rate(minio_http_requests_duration_seconds_sum{request_type="GET", service="{{ template "minio.fullname" . }}"}[{{ .Values.minio.alertConfigurations.getLatencyAlert.timeRange }}]))/sum by (service) ((rate(minio_http_requests_duration_seconds_count{request_type="GET", service="{{ template "minio.fullname" . }}"}[{{ .Values.minio.alertConfigurations.getLatencyAlert.timeRange }}]))) > {{ .Values.minio.alertConfigurations.getLatencyAlert.letancyThreshold }}
        for: 10s
        labels:
          severity: minor
        annotations:
          summary: Average GET request latency of minio service {{ "{{" }} $labels.service {{ "}}" }} exceeded configured threshold in last {{ .Values.minio.alertConfigurations.getLatencyAlert.timeRange }}. If the problem persists, escalate to support.
      - alert: minio-post_latency_alert-{{ .Release.Name }}
        expr: sum by (service) (rate(minio_http_requests_duration_seconds_sum{request_type="POST", service="{{ template "minio.fullname" . }}"}[{{ .Values.minio.alertConfigurations.postLatencyAlert.timeRange }}]))/sum by (service) ((rate(minio_http_requests_duration_seconds_count{request_type="POST", service="{{ template "minio.fullname" . }}"}[{{ .Values.minio.alertConfigurations.postLatencyAlert.timeRange }}]))) > {{ .Values.minio.alertConfigurations.postLatencyAlert.letancyThreshold }}
        for: 10s
        labels:
          severity: minor
        annotations:
          summary: Average POST request latency of minio service {{ "{{" }} $labels.service {{ "}}" }} exceeded configured threshold in last {{ .Values.minio.alertConfigurations.postLatencyAlert.timeRange }}. If the problem persists, escalate to support.
      - alert: minio-disk_offline_alert-{{ .Release.Name }}
        expr: (sum by (service) (minio_offline_disks{service="{{ template "minio.fullname" . }}"})) > 0 
        for: 10s
        labels:
          severity: critical
        annotations:
          summary: one or more persistent storage disk associated with minio service {{ "{{" }} $labels.service {{ "}}" }} are offline. Escalate to Kubernetes cluster administrator.
      - alert: minio-disk_failure_alert-{{ .Release.Name }}
        expr: (sum by (service) (minio_offline_disks{service="{{ template "minio.fullname" . }}"})/sum by (service) (minio_total_disks{service="{{ template "minio.fullname" . }}"})*100) >= 50
        for: 10s
        labels:
          severity: fatal
        annotations:
          summary: More than 50% persistent storage disks associated with minio service {{ "{{" }} $labels.service {{ "}}" }} are offline. Escalate to Kubernetes cluster administrator.
      - alert: minio-503_alert-{{ .Release.Name }}
        expr: sum by (service) (round(increase(promhttp_metric_handler_requests_total{code="503", service="{{ template "minio.fullname" . }}"}[{{ .Values.minio.alertConfigurations.serviceNotAvailableAlert.timeRange }}]))) > {{ .Values.minio.alertConfigurations.serviceNotAvailableAlert.countThreshold }}  
        for: 10s
        labels:
          severity: major
        annotations:
          summary: Internal minio server error. If the problem persists, collect diagnotics logs and escalate to support or contact minio admin.
{{- end}}