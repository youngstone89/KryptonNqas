{{- if eq .Values.prometheusOperator.enabled false}}
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ template "clag.fullname" . }}-alerts
  annotations:
{{ toYaml .Values.alertConfiguration.configmap.annotations | indent 4 }}
  labels:
{{ toYaml .Values.prometheus.alerts.selectorLabels | indent 4 }}
data:
  prometheus.rules.yaml: |-
      groups:
      - name: ./clagAlert.rules
        rules:
        - alert: clag-error_response-{{ .Release.Name }}
          expr: (sum by(service) (round(increase(clag_error_response_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.errorResponseAlert.timeRange }}]))) / sum by(service) (round(increase(clag_response_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.errorResponseAlert.timeRange }}])))) * 100 >= {{ .Values.alertConfiguration.errorResponseAlert.percentageThreshold }}
          for: 10s
          labels:
            severity: critical
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Error responses from Call Log Aggregator were observed for {{ "{{" }} $labels.service {{ "}}" }} service. If the problem persists, collect diagnotics logs and escalate to support.
        - alert: clag-request_validation_failure-{{ .Release.Name }}
          expr: (sum by(service) (round(increase(clag_request_validation_failure_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.validationFailureAlert.timeRange }}]))) / sum by(service) (round(increase(clag_response_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.validationFailureAlert.timeRange }}])))) * 100 >= {{ .Values.alertConfiguration.validationFailureAlert.percentageThreshold }}
          for: 10s
          labels:
            severity: critical
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Request validation errors were observed for {{ "{{" }} $labels.service {{ "}}" }} service. Verify that client is sending a valid request payload as per the protocol specification. 
        - alert: clag-failed_archive_and_export-{{ .Release.Name }}
          expr: (sum by(service) ((round(increase(clag_failed_archive_and_export_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.archiveAndExportFailureAlert.timeRange }}]))) / ((round(increase(clag_failed_archive_and_export_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.archiveAndExportFailureAlert.timeRange }}]))) + (round(increase(clag_successful_archive_and_export_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.archiveAndExportFailureAlert.timeRange }}])))))) * 100 >= {{ .Values.alertConfiguration.archiveAndExportFailureAlert.percentageThreshold }}
          for: 10s
          labels:
            severity: critical
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Archive and export to object store failures were observed for {{ "{{" }} $labels.service {{ "}}" }} service. Verify Minio Client settings in Coll Log Aggregator configmap. If the problem persists, collect diagnotics logs and escalate to support.
        - alert: clag-runtime_config_watcher_init_failure-{{ .Release.Name }}
          expr: sum by (service) (clag_runtime_configmap_watcher_init_failure_current{service="{{ template "clag.fullname" . }}"}) > 0
          for: 10s
          labels:
            severity: error
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Runtime config watcher initialization failed for {{ "{{" }} $labels.service {{ "}}" }} service. Collect diagnotics logs and escalate to support.
        - alert: clag-incomplete_sessions-{{ .Release.Name }}
          expr: (sum by(service) (round(increase(clag_incomplete_sessions_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.incompleteSessionsAlert.timeRange }}]))) / sum by(service) (round(increase(clag_sessions_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.incompleteSessionsAlert.timeRange }}])))) * 100 >= {{ .Values.alertConfiguration.incompleteSessionsAlert.percentageThreshold }}
          for: 10s
          labels:
            severity: critical
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Incomplete sessions are observred for {{ "{{" }} $labels.service {{ "}}" }} service. This happens when Client connecting to Call log aggregator is disconnecting the websocket connection before session close command. If the problem persists, Collect diagnotics logs and escalate to support.
        - alert: clag-file_persist_error-{{ .Release.Name }}
          expr: (sum by(service) ((round(increase(clag_file_persist_error_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.filePersistError.timeRange }}]))) / ((round(increase(clag_file_persist_error_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.filePersistError.timeRange }}]))) + (round(increase(clag_file_persist_total{service="{{ template "clag.fullname" . }}"}[{{ .Values.alertConfiguration.filePersistError.timeRange }}])))))) * 100 >= {{ .Values.alertConfiguration.filePersistError.percentageThreshold }}
          for: 10s
          labels:
            severity: error
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Audio and Metadata persistence failures were observed for {{ "{{" }} $labels.service {{ "}}" }} service. Verify read write access permissions are set correctly for Persistent Volumes attached to CLAG pods.
        - alert: clag-pod_container_nqas_restart-{{ .Release.Name }}
          expr: sum(round(increase(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace="{{ .Release.Namespace }}", pod=~"{{ template "clag.fullname" . }}.*",container="{{ template "clag-container.name" . }}"}[{{ .Values.alertConfiguration.podRestart.timeRange }}]))) > {{ .Values.alertConfiguration.podRestart.avgThresholdCount }}
          for: {{ .Values.alertConfiguration.podRestart.timeInterval }}
          labels:
            severity: critical
            {{ toYaml .Values.alertConfiguration.alertLabels | nindent 12 }}
          annotations:
            summary: Frequent container restart observed for service {{ template "clag.fullname" . }} and for container  {{ template "clag-container.name" . }} . 
{{- end}}